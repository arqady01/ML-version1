# PAC

PAC（Probably Approximately Correct）概率近似正确

我们希望结果值和真实值越接近越好，用数学可以表述为：$f(x) - y \le \epsilon$ ，当 $\epsilon$ 很小很小的时候，说明模型的预测结果效果很好；当 $\epsilon$ 完全等于0，说明模型输出结果就是真实的。

我们希望得到的是一个 $f(x) - y \to 0$ 的模型，但真实情况是并不是每次都能得到，只是一个概率能得到，所以我们希望这个概率越高越好，数学表达为：$P(\left | f(x) - y \right | \le \epsilon) \ge 1 - \delta$ ，当 $\delta$ 非常小时，概率逼近1；当 $\delta$ 等于0时，概率为1

# 归纳偏好 & NFL

**奥卡姆剃刀**：若无必要，勿增实体。

我们知道，任何一个机器学习算法都必有其偏好。在ML中，奥卡姆剃刀主要体现在模型的选择与泛化能力上，比如：在多个能拟合训练数据的模型中，倾向于选择结构更简单、参数更少的模型。

归纳偏好：从有限样本中学习时，对假设空间中某些假设的倾向性或选择的规则。

举个例子，$f(x) = a^2x + bx +c$ 和 $f(x) = a^3x + c$，你倾向于选择多参但低幂（二次）还是高幂但少参（a、c）的模型，这个要看实际的业务需求

NFL：一个算法 $L_a$ 在某些问题上比另一个算法 $L_b$ 好，必然存在一些问题 $L_b$ 比 $L_a$ 好

但是实际情况并非如此，因为我们只需关心自己正在解决的问题，脱离实际问题，谈论“什么算法更好”毫无意义。

# 泛化能力

- 泛化误差：在未来（未知）样本上的误差，并非已知样本上的误差，不然用红黑树或哈希表存起来多好，还零误差呢
- 经验误差：在训练集上的误差，或者叫做训练误差

考虑两个问题：经验误差是否越小越好，泛化误差是否越小越好？

## 欠拟合

模型过于简单，未能充分学习训练数据中蕴含的基本规律和模式。

## 过拟合（overfitting）

模型过于复杂，过度学习了训练数据中的细节和随机噪声，以至于把训练数据中的偶然特征也当成了普适规律。

想象一下，给人喂了100张图片训练识别不同品种的狗，太粗心的人只记住了“四条腿、有尾巴”，以至于他看到猫也说是狗，太认真的人不仅记住了每个品种的特征，还记住了每张照片的细节“这是金毛，因为它站在红色沙发前”，100张照片能认对，但同一只金毛换个背景就不认识了，因为记住了太多无关细节，失去了泛化能力。

我们做机器学习的挑战从这个角度解释，就是从有限的训练集中找到不欠拟合又不过拟合的平衡点，所以我们再学到经典机器学习算法的时候，都要问自己，这个算法是如何缓解过拟合的，缓解的技术在什么情况下又失效了
